{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Website Crawler & PDF Generator\n",
    "\n",
    "This notebook crawls a website, analyzes content using AI, and generates professional PDFs with summaries and project ideas for each module.\n",
    "\n",
    "## Features\n",
    "- üï∑Ô∏è Crawls websites and extracts content from all pages\n",
    "- ü§ñ Uses Claude AI to generate intelligent summaries\n",
    "- üìã Identifies buildable projects from documentation\n",
    "- üìÑ Creates professional PDF reports for each module\n",
    "- üé® Beautiful formatting with tables and sections\n",
    "\n",
    "## How to Use\n",
    "1. Run the setup cell to install dependencies\n",
    "2. Configure your settings (URL, API key, max pages)\n",
    "3. Run the crawler and generator\n",
    "4. Download your PDFs from the 'pdfs' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install beautifulsoup4 requests reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Import Libraries and Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# PDF generation\n",
    "from reportlab.lib.pagesizes import letter, A4\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_JUSTIFY\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteCrawler:\n",
    "    \"\"\"Crawls a website and extracts content from all internal pages.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, max_pages=50):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.visited_urls = set()\n",
    "        self.pages_content = []\n",
    "        self.domain = urlparse(base_url).netloc\n",
    "        \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is valid and belongs to the same domain.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return bool(parsed.netloc) and parsed.netloc == self.domain\n",
    "    \n",
    "    def get_all_links(self, url, soup):\n",
    "        \"\"\"Extract all internal links from a page.\"\"\"\n",
    "        links = set()\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = urljoin(url, href)\n",
    "            clean_url = full_url.split('#')[0].split('?')[0]\n",
    "            \n",
    "            if self.is_valid_url(clean_url) and clean_url not in self.visited_urls:\n",
    "                links.add(clean_url)\n",
    "        return links\n",
    "    \n",
    "    def extract_content(self, soup, url):\n",
    "        \"\"\"Extract meaningful content from a page.\"\"\"\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        title = soup.find('title')\n",
    "        title = title.get_text().strip() if title else \"Untitled\"\n",
    "        \n",
    "        main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "        \n",
    "        if main_content:\n",
    "            text = main_content.get_text(separator='\\n', strip=True)\n",
    "            text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "            \n",
    "            headings = []\n",
    "            for heading in main_content.find_all(['h1', 'h2', 'h3']):\n",
    "                headings.append({\n",
    "                    'level': heading.name,\n",
    "                    'text': heading.get_text().strip()\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': text[:5000],\n",
    "                'headings': headings,\n",
    "                'word_count': len(text.split())\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Crawl the website starting from base_url.\"\"\"\n",
    "        print(f\"üï∑Ô∏è  Starting crawl of {self.base_url}\")\n",
    "        print(f\"   Maximum pages: {self.max_pages}\\n\")\n",
    "        \n",
    "        urls_to_visit = {self.base_url}\n",
    "        \n",
    "        while urls_to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            url = urls_to_visit.pop()\n",
    "            \n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"üìÑ Crawling ({len(self.visited_urls) + 1}/{self.max_pages}): {url[:80]}...\")\n",
    "                \n",
    "                response = requests.get(url, timeout=10, headers={\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                })\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                content = self.extract_content(soup, url)\n",
    "                \n",
    "                if content:\n",
    "                    self.pages_content.append(content)\n",
    "                \n",
    "                self.visited_urls.add(url)\n",
    "                new_links = self.get_all_links(url, soup)\n",
    "                urls_to_visit.update(new_links)\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error: {str(e)}\")\n",
    "                self.visited_urls.add(url)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Crawl complete! Visited {len(self.visited_urls)} pages\")\n",
    "        return self.pages_content\n",
    "\n",
    "\n",
    "class ContentAnalyzer:\n",
    "    \"\"\"Analyzes content using Claude AI to generate summaries and project ideas.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        self.api_key = api_key\n",
    "        self.use_ai = bool(api_key)\n",
    "        if not self.use_ai:\n",
    "            print(\"‚ö†Ô∏è  No API key provided. Using basic analysis.\")\n",
    "    \n",
    "    def organize_by_modules(self, pages_content):\n",
    "        \"\"\"Organize pages into logical modules based on URL structure.\"\"\"\n",
    "        modules = defaultdict(list)\n",
    "        \n",
    "        for page in pages_content:\n",
    "            parsed = urlparse(page['url'])\n",
    "            path_parts = [p for p in parsed.path.split('/') if p]\n",
    "            \n",
    "            if not path_parts:\n",
    "                module_name = \"Home\"\n",
    "            else:\n",
    "                module_name = path_parts[0].replace('-', ' ').replace('_', ' ').title()\n",
    "            \n",
    "            modules[module_name].append(page)\n",
    "        \n",
    "        return dict(modules)\n",
    "    \n",
    "    def analyze_with_ai(self, module_name, pages):\n",
    "        \"\"\"Use Claude AI to analyze content and generate insights.\"\"\"\n",
    "        if not self.use_ai:\n",
    "            return self._basic_analysis(module_name, pages)\n",
    "        \n",
    "        try:\n",
    "            content_summary = f\"Module: {module_name}\\n\\n\"\n",
    "            for i, page in enumerate(pages[:10], 1):\n",
    "                content_summary += f\"Page {i}: {page['title']}\\n\"\n",
    "                content_summary += f\"Content: {page['content'][:500]}...\\n\\n\"\n",
    "            \n",
    "            response = requests.post(\n",
    "                'https://api.anthropic.com/v1/messages',\n",
    "                headers={\n",
    "                    'Content-Type': 'application/json',\n",
    "                    'x-api-key': self.api_key,\n",
    "                    'anthropic-version': '2023-06-01'\n",
    "                },\n",
    "                json={\n",
    "                    'model': 'claude-sonnet-4-20250514',\n",
    "                    'max_tokens': 2000,\n",
    "                    'messages': [{\n",
    "                        'role': 'user',\n",
    "                        'content': f\"\"\"Analyze this website module and provide:\n",
    "\n",
    "1. A concise summary (2-3 paragraphs)\n",
    "2. 3-5 specific projects that could be built\n",
    "3. Key technologies/concepts\n",
    "\n",
    "{content_summary}\n",
    "\n",
    "Format as JSON with keys: summary, buildable_projects (array), key_concepts (array)\"\"\"\n",
    "                    }]\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                content_text = result['content'][0]['text']\n",
    "                content_text = re.sub(r'```json\\s*|\\s*```', '', content_text).strip()\n",
    "                return json.loads(content_text)\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  API Error: {response.status_code}\")\n",
    "                return self._basic_analysis(module_name, pages)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  AI Error: {str(e)}\")\n",
    "            return self._basic_analysis(module_name, pages)\n",
    "    \n",
    "    def _basic_analysis(self, module_name, pages):\n",
    "        \"\"\"Fallback basic analysis without AI.\"\"\"\n",
    "        total_words = sum(p['word_count'] for p in pages)\n",
    "        all_text = ' '.join([p['content'] for p in pages])\n",
    "        words = re.findall(r'\\b[a-z]{4,}\\b', all_text.lower())\n",
    "        word_freq = defaultdict(int)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        \n",
    "        common_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        \n",
    "        return {\n",
    "            'summary': f\"This '{module_name}' module contains {len(pages)} pages with ~{total_words} words. \"\n",
    "                      f\"Key topics: {', '.join([w[0] for w in common_words[:5]])}.\",\n",
    "            'buildable_projects': [\n",
    "                f\"Documentation site for {module_name}\",\n",
    "                f\"Tutorial application based on {module_name}\",\n",
    "                f\"Reference implementation tool\"\n",
    "            ],\n",
    "            'key_concepts': [w[0] for w in common_words[:5]]\n",
    "        }\n",
    "\n",
    "\n",
    "class PDFGenerator:\n",
    "    \"\"\"Generates professional PDFs with summaries and project ideas.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir='./pdfs'):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.styles = getSampleStyleSheet()\n",
    "        self._setup_custom_styles()\n",
    "    \n",
    "    def _setup_custom_styles(self):\n",
    "        self.styles.add(ParagraphStyle(\n",
    "            name='CustomTitle',\n",
    "            parent=self.styles['Title'],\n",
    "            fontSize=24,\n",
    "            textColor=colors.HexColor('#1e40af'),\n",
    "            spaceAfter=30,\n",
    "            alignment=TA_CENTER\n",
    "        ))\n",
    "        \n",
    "        self.styles.add(ParagraphStyle(\n",
    "            name='SectionHeading',\n",
    "            parent=self.styles['Heading2'],\n",
    "            fontSize=14,\n",
    "            textColor=colors.HexColor('#dc2626'),\n",
    "            spaceBefore=12,\n",
    "            spaceAfter=6\n",
    "        ))\n",
    "    \n",
    "    def generate_pdf(self, module_name, analysis, pages, base_url):\n",
    "        filename = f\"{module_name.lower().replace(' ', '_')}_analysis.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter,\n",
    "                              topMargin=0.75*inch, bottomMargin=0.75*inch)\n",
    "        story = []\n",
    "        \n",
    "        # Title\n",
    "        story.append(Paragraph(f\"Module: {module_name}\", self.styles['CustomTitle']))\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        story.append(Paragraph(\"Analysis Report\", self.styles['Heading2']))\n",
    "        story.append(Spacer(1, 0.1*inch))\n",
    "        story.append(Paragraph(f\"Generated: {datetime.now().strftime('%B %d, %Y')}\", \n",
    "                             self.styles['Normal']))\n",
    "        story.append(Paragraph(f\"Source: {base_url}\", self.styles['Normal']))\n",
    "        story.append(Spacer(1, 0.3*inch))\n",
    "        \n",
    "        # Summary\n",
    "        story.append(Paragraph(\"üìã Summary\", self.styles['SectionHeading']))\n",
    "        story.append(Spacer(1, 0.1*inch))\n",
    "        story.append(Paragraph(analysis.get('summary', 'No summary available.'), \n",
    "                             self.styles['Normal']))\n",
    "        story.append(Spacer(1, 0.3*inch))\n",
    "        \n",
    "        # Projects\n",
    "        story.append(Paragraph(\"üöÄ Things You Can Build\", self.styles['SectionHeading']))\n",
    "        story.append(Spacer(1, 0.1*inch))\n",
    "        \n",
    "        projects = analysis.get('buildable_projects', [])\n",
    "        for i, project in enumerate(projects, 1):\n",
    "            story.append(Paragraph(f\"{i}. {project}\", self.styles['Normal']))\n",
    "            story.append(Spacer(1, 0.05*inch))\n",
    "        \n",
    "        story.append(Spacer(1, 0.3*inch))\n",
    "        \n",
    "        # Concepts\n",
    "        story.append(Paragraph(\"üí° Key Concepts\", self.styles['SectionHeading']))\n",
    "        story.append(Spacer(1, 0.1*inch))\n",
    "        concepts = \", \".join(analysis.get('key_concepts', []))\n",
    "        story.append(Paragraph(concepts or \"None identified\", self.styles['Normal']))\n",
    "        story.append(Spacer(1, 0.3*inch))\n",
    "        \n",
    "        # Pages table\n",
    "        story.append(PageBreak())\n",
    "        story.append(Paragraph(\"üìö Pages in Module\", self.styles['SectionHeading']))\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        \n",
    "        table_data = [['#', 'Title', 'Words']]\n",
    "        for i, page in enumerate(pages[:20], 1):\n",
    "            table_data.append([\n",
    "                str(i),\n",
    "                Paragraph(page['title'][:60], self.styles['Normal']),\n",
    "                str(page['word_count'])\n",
    "            ])\n",
    "        \n",
    "        t = Table(table_data, colWidths=[0.5*inch, 5*inch, 1*inch])\n",
    "        t.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('FONTSIZE', (0, 0), (-1, 0), 10),\n",
    "            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "        ]))\n",
    "        story.append(t)\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"   ‚úÖ Generated: {filename}\")\n",
    "        return filepath\n",
    "\n",
    "print(\"‚úÖ All classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Configure Your Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "WEBSITE_URL = \"https://docs.python.org\"  # Change this to your target website\n",
    "MAX_PAGES = 30  # Maximum number of pages to crawl\n",
    "ANTHROPIC_API_KEY = \"\"  # Optional: Add your Anthropic API key for AI-powered analysis\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   Website: {WEBSITE_URL}\")\n",
    "print(f\"   Max Pages: {MAX_PAGES}\")\n",
    "print(f\"   AI Analysis: {'Enabled' if ANTHROPIC_API_KEY else 'Disabled (using basic analysis)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Run the Crawler and Generate PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Crawl the website\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Website Analysis...\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "crawler = WebsiteCrawler(WEBSITE_URL, max_pages=MAX_PAGES)\n",
    "pages_content = crawler.crawl()\n",
    "\n",
    "if not pages_content:\n",
    "    print(\"‚ùå No content extracted. Please check the URL.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Extracted content from {len(pages_content)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Organize into modules\n",
    "if pages_content:\n",
    "    analyzer = ContentAnalyzer(api_key=ANTHROPIC_API_KEY if ANTHROPIC_API_KEY else None)\n",
    "    modules = analyzer.organize_by_modules(pages_content)\n",
    "    \n",
    "    print(f\"\\nüìä Organized into {len(modules)} modules:\")\n",
    "    for module_name, pages in modules.items():\n",
    "        print(f\"   ‚Ä¢ {module_name}: {len(pages)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Analyze and generate PDFs\n",
    "if pages_content:\n",
    "    pdf_generator = PDFGenerator()\n",
    "    generated_pdfs = []\n",
    "    \n",
    "    print(\"\\nüî¨ Analyzing modules and generating PDFs...\\n\")\n",
    "    \n",
    "    for module_name, pages in modules.items():\n",
    "        print(f\"üìù Processing: {module_name}\")\n",
    "        analysis = analyzer.analyze_with_ai(module_name, pages)\n",
    "        pdf_path = pdf_generator.generate_pdf(module_name, analysis, pages, WEBSITE_URL)\n",
    "        generated_pdfs.append(pdf_path)\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ú® All done!\")\n",
    "    print(f\"Generated {len(generated_pdfs)} PDF reports\")\n",
    "    print(\"\\nPDF files:\")\n",
    "    for pdf in generated_pdfs:\n",
    "        print(f\"   üìÑ {os.path.basename(pdf)}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 5: Download PDFs\n",
    "\n",
    "Your PDFs are saved in the `pdfs` folder. In Google Colab:\n",
    "1. Click the folder icon on the left sidebar\n",
    "2. Navigate to the `pdfs` folder\n",
    "3. Right-click any PDF and select \"Download\"\n",
    "\n",
    "Or run the cell below to zip all PDFs for easy download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all PDFs for easy download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "if os.path.exists('pdfs') and os.listdir('pdfs'):\n",
    "    shutil.make_archive('website_analysis_pdfs', 'zip', 'pdfs')\n",
    "    print(\"üì¶ PDFs zipped successfully!\")\n",
    "    print(\"‚¨áÔ∏è  Downloading...\")\n",
    "    files.download('website_analysis_pdfs.zip')\n",
    "else:\n",
    "    print(\"‚ùå No PDFs found. Please run the analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Optional: View Sample Analysis\n",
    "\n",
    "Run this cell to see a preview of one module's analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample analysis\n",
    "if 'modules' in locals() and modules:\n",
    "    sample_module = list(modules.keys())[0]\n",
    "    sample_pages = modules[sample_module]\n",
    "    \n",
    "    print(f\"üìñ Sample Analysis for: {sample_module}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    sample_analysis = analyzer.analyze_with_ai(sample_module, sample_pages)\n",
    "    \n",
    "    print(f\"\\nüìã Summary:\")\n",
    "    print(sample_analysis['summary'])\n",
    "    \n",
    "    print(f\"\\nüöÄ Buildable Projects:\")\n",
    "    for i, project in enumerate(sample_analysis['buildable_projects'], 1):\n",
    "        print(f\"   {i}. {project}\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Concepts:\")\n",
    "    print(f\"   {', '.join(sample_analysis['key_concepts'])}\")\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
